{"cells":[{"cell_type":"markdown","metadata":{"id":"tbTYFRhJoaBu"},"source":["# Cifar10 classification tricks\n","\n","In this notebook you will download the cifar10 dataset which contains quite small images (32x32x3) of 10 classes. The data is from the Canadian Institute For Advanced Research. You will plot examples of the images with the class label. Note that because the images are so small it is not always very easy to recognise which of the ten classes is on the image, even as a human. After loading the dataset you will train multiple models and compare the performances of the models on the testset.\n","\n","**Dataset:**  You work with the Cifar10 dataset. You have 60'000 32x32 pixel color images of 10 classes (\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\")\n","\n","**Content:**\n","* load the original cifar10 data create a train val and test dataset\n","* visualize samples of cifar10 dataset\n","\n","* train a random forest on the pixelvalues\n","* train a cnn from scratch without normalization\n","* train a cnn from scratch with normalization\n","* train a cnn from scratch with dropout\n","* train a cnn from scratch with batchnorm\n","* train a cnn from scratch with data augmentation\n","\n","* compare the performances of the models\n","\n","\n","\n","\n","\n","\n","* ðŸ”§  your task at the end of the notebook: try to beat the models from this notebook\n"]},{"cell_type":"markdown","metadata":{"id":"PEIS4WvpsT5t"},"source":["#### Imports\n","\n","In the next two cells, we load all the required libraries and functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0u353ydb9w2","scrolled":true},"outputs":[],"source":["# load required libraries:\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')\n","from sklearn.metrics import confusion_matrix\n","\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"torch\"\n","import keras\n","import torch # not needed yet\n","\n","print(f'Keras_version: {keras.__version__}')# 3.5.0\n","print(f'torch_version: {torch.__version__}')# 2.5.1+cu121\n","print(f'keras backend: {keras.backend.backend()}')\n","\n","# Keras Building blocks\n","from keras.models import Sequential\n","from keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation,Dropout,BatchNormalization\n","from keras.optimizers import SGD\n","from keras.utils import to_categorical\n","from keras import optimizers\n","\n","\n","# dataloader and augmentation strategy\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchvision import transforms as tr\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"px1v5TEDtWGW"}},{"cell_type":"markdown","source":["### Dataloader"],"metadata":{"id":"BSEPI14itYH9"}},{"cell_type":"code","source":["class NumpyDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image = self.data[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","\n","\n","def plot_history(history):\n","  # plot the development of the accuracy and loss during training\n","  plt.figure(figsize=(12,4))\n","  plt.subplot(1,2,(1))\n","  plt.plot(history.history['accuracy'],linestyle='-.')\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'valid'], loc='lower right')\n","  plt.subplot(1,2,(2))\n","  plt.plot(history.history['loss'],linestyle='-.')\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'valid'], loc='upper right')"],"metadata":{"id":"ElSbrpqttqS4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ThLhUQVgtpp6"}},{"cell_type":"markdown","metadata":{"id":"Y7kfDCYsxRVJ"},"source":["\n","### Load and plot the data\n","\n","In the next cell you will load the Cifar10 dataset, 50'000 images are in the training set and 10'000 are in the test dataset. You will use 10'000 for the train and validation dataset.\n","You will plot one random example of each label and will see\n","that the images are really small and finally you can convert the lables into the one hot encoding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRLfhCzVviwL"},"outputs":[],"source":["from keras.datasets import cifar10\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zwRlaILb9xF"},"outputs":[],"source":["# separate train val and test dataset\n","X_train=x_train[0:10000]\n","Y_train=to_categorical(y_train[0:10000],10) # one-hot encoding\n","\n","X_val=x_train[20000:30000]\n","Y_val=to_categorical(y_train[20000:30000],10)\n","\n","X_test=x_test\n","Y_test=to_categorical(y_test,10)\n","\n","del x_train, y_train, x_test, y_test\n","\n","\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6jukEcMb9xL"},"outputs":[],"source":["labels=np.array([\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"])\n","#sample image of each label\n","plt.figure(figsize=(15,15))\n","for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n","    rmd=np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n","    plt.subplot(1,10,i+1)\n","    img=X_train[rmd]\n","    plt.imshow(img[0,:,:,:])\n","    plt.title(labels[i]+\" \"+str(np.argmax(Y_train,axis=1)[rmd][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLX7w_4Zb9xP"},"outputs":[],"source":["# check the shape of the data\n","X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"]},{"cell_type":"markdown","metadata":{"id":"z5rxBKcRdPKR"},"source":["### RF on pixelvalues\n","In this section you will train a random forest on the raw pixelvalues of the images.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQnu_hIpcUp1"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(n_estimators=40,random_state=22)\n","clf.fit(X_train.reshape(len(X_train),32*32*3), np.argmax(Y_train,axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kY9AkXRrcUp-"},"outputs":[],"source":["pred=clf.predict(X_test.reshape(len(X_test),32*32*3))\n","acc=np.average(pred==np.argmax(Y_test,axis=1))\n","res1 = pd.DataFrame(\n","          {'Acc' : acc}, index=['rf on pixelvalues'])\n","res1"]},{"cell_type":"markdown","metadata":{"id":"mi-o6-OagtZf"},"source":["### CNN from scratch without normalization\n","In this section you train a cnn from scratch to learn to classify the images into the right label. Normalization is not applied to the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlXRWiwi8FHq"},"outputs":[],"source":["model  =  Sequential()\n","\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(500))\n","model.add(Activation('relu'))\n","model.add(Dense(300))\n","model.add(Activation('relu'))\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","source":["\n","# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=None)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=None)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=None)\n","\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","images, labels = next(iter(train_dataloader))\n","print(f'minimum value {images.min()} | maximum value {images.max()}')"],"metadata":{"id":"ffOzBlrnUHSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history =model.fit(train_dataloader, epochs=10, validation_data=val_dataloader)"],"metadata":{"id":"k8p76XlEYUJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aud3gKbNhT3b"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYco3ZFohT3r"},"outputs":[],"source":["acc=np.average(np.argmax(model.predict(test_dataloader),axis=1)==np.argmax(Y_test,axis=1))\n","res2 = pd.DataFrame(\n","          {'Acc' : acc}, index=['cnn from scratch without normalization']\n",")\n","pd.concat([res1,res2])"]},{"cell_type":"markdown","metadata":{"id":"iDqf88OmQ8mg"},"source":["### CNN from scratch with normalization\n","In this section you train a cnn from scratch to learn to classify the images into the right label. Normalization is applied to the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrjY7zcwQ8mh"},"outputs":[],"source":["model  =  Sequential()\n","\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(500))\n","model.add(Activation('relu'))\n","model.add(Dense(300))\n","model.add(Activation('relu'))\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZvfCUkJQ8mh"},"outputs":[],"source":["#  these transformatins make sure that the data leis in 0....1 range\n","train_transforms = tr.Compose([\n","    tr.ToPILImage(),               #  Convert numpy array to PIL image\n","    tr.ToTensor(),                 # Convert PIL image to Tensor\n","    tr.Lambda(lambda x: x.permute(1, 2, 0))  # Convert (C, H, W) -> (H, W, C)\n","])\n","\n","val_test_transforms = tr.Compose([\n","    tr.ToPILImage(),               # Converting numpy array to PIL image\n","    tr.ToTensor(),                 # Convert PIL image to Tensor\n","    tr.Lambda(lambda x: x.permute(1, 2, 0))  # Convert (C, H, W) -> (H, W, C)\n","])\n","\n","\n","# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=train_transforms)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=val_test_transforms)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=val_test_transforms)\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","images, labels = next(iter(train_dataloader))\n","print(f'minimum value {images.min()} | maximum value {images.max()}')\n"]},{"cell_type":"code","source":["history =model.fit(train_dataloader, epochs=10, validation_data=val_dataloader)"],"metadata":{"id":"BTGRlcwNbHgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43AMELzvQ8mh"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMcWyUOWQ8mh"},"outputs":[],"source":["acc=np.average(np.argmax(model.predict(test_dataloader),axis=1)==np.argmax(Y_test,axis=1))\n","res3 = pd.DataFrame(\n","          {'Acc' : acc}, index=['cnn from scratch with normalization']\n",")\n","pd.concat([res1,res2,res3])"]},{"cell_type":"markdown","metadata":{"id":"wIZH1xpehpOH"},"source":["### CNN from scratch with Dropout\n","In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use dropout layers in the classification part. Normalization is not used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYD-TYOzhpOM"},"outputs":[],"source":["model  =  Sequential()\n","\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Flatten())\n","model.add(Dropout(0.3))   #<------ Droput layers\n","model.add(Dense(500))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.3))   #<------ Droput layers\n","model.add(Dense(300))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.3))   #<------ Droput layers\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","source":["# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=None)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=None)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=None)\n","\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","images, labels = next(iter(train_dataloader))\n","print(f'minimum value {images.min()} | maximum value {images.max()}')"],"metadata":{"id":"vWV2M4XObU_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfv_8EYChpOR"},"outputs":[],"source":["history =model.fit(train_dataloader, epochs=15, validation_data=val_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4utU_Wd7hpOY"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qAipwYkhpOf"},"outputs":[],"source":["acc = np.average(np.argmax(model.predict(test_dataloader),axis=1)==np.argmax(Y_test,axis=1))\n","res4 = pd.DataFrame(\n","          {'Acc' : acc}, index=['cnn from scratch with dropout']\n",")\n","pd.concat([res1,res2,res3,res4])"]},{"cell_type":"markdown","metadata":{"id":"qP_oH_fcih0D"},"source":["### CNN from scratch with Batchnorm\n","In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use batchnorm on the input and in the convolutional part of the network. Note that we use the original images and do not normalize them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLQ-RZgxih0J"},"outputs":[],"source":["model  =  Sequential()\n","\n","model.add(BatchNormalization(input_shape=(32,32,3)))\n","model.add(Convolution2D(16,(3,3),padding=\"same\"))\n","model.add(BatchNormalization())                    #<------ Batchnorm layers\n","model.add(Activation('relu'))\n","model.add(Convolution2D(16,(3,3),padding=\"same\"))\n","model.add(BatchNormalization())                    #<------ Batchnorm layers\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Convolution2D(32,(3,3),padding=\"same\"))\n","model.add(BatchNormalization())                    #<------ Batchnorm layers\n","model.add(Activation('relu'))\n","model.add(Convolution2D(32,(3,3),padding=\"same\"))\n","model.add(BatchNormalization())                    #<------ Batchnorm layers\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(500))\n","model.add(Activation('relu'))\n","model.add(Dense(300))\n","model.add(Activation('relu'))\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","source":["# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=None)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=None)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=None)\n","\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","images, labels = next(iter(train_dataloader))\n","print(f'minimum value {images.min()} | maximum value {images.max()}')"],"metadata":{"id":"5uAZkjETcoNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f5HV12Pih0Q"},"outputs":[],"source":["history =model.fit(train_dataloader, epochs=15, validation_data=val_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ar8YM_44ih0Y"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgqm4Erdih0e"},"outputs":[],"source":["acc = np.average(np.argmax(model.predict(test_dataloader),axis=1)==np.argmax(Y_test,axis=1))\n","res5 = pd.DataFrame(\n","          {'Acc' : acc}, index=['cnn from scratch with batchnorm']\n",")\n","pd.concat([res1,res2,res3,res4,res5])"]},{"cell_type":"markdown","metadata":{"id":"y12h3tb5g4r4"},"source":["#### Exercise\n","Calculate the confusion matrix of the networks.  \n","Play around with the dropout rate and the position of the batchnorm."]},{"cell_type":"markdown","metadata":{"id":"XiKVO-uOlMbg"},"source":["### CNN from scratch with Data Augmentation\n","In this section you train a cnn from scratch to learn to classify the images into the right label. This time you will use data augmentation, so the network will train on slightly different versions of the images in each epoch.\n","\n","Data Augmentation is especially helpful if you do not have lots of data. Another approach to train with few data is to use a pretrained neural network. This will be covered in the next notebook.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXFfkFM5lMbn"},"outputs":[],"source":["model  =  Sequential()\n","\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(32,32,3)))\n","model.add(Convolution2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(Convolution2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(500))\n","model.add(Activation('relu'))\n","model.add(Dense(300))\n","model.add(Activation('relu'))\n","model.add(Dense(100))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","    optimizer='adam',\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()"]},{"cell_type":"markdown","source":["Note that for the augmentation we now need Dataloaders from torch.\n","It creates a dataset which can be loaded during traing , with the transformations we can specify what kind of augmentation each image should recieve."],"metadata":{"id":"5xblx3KxG8h9"}},{"cell_type":"code","source":["# Transformations pipeline for training\n","train_transforms = tr.Compose([\n","    tr.ToPILImage(),               #  Convert numpy array to PIL image\n","    tr.ToTensor(),                 # Convert PIL image to Tensor\n","    tr.RandomHorizontalFlip(),                                                           # Random horizontal flip\n","    tr.RandomRotation(15),                                                              # Random rotation\n","    tr.RandomCrop(32, padding=4),                                                       # Random cropping with padding\n","    tr.Lambda(lambda x: (x * 255.0).clamp(0, 255).permute(1, 2, 0))                     # Scale back to [0, 255] and return to (H, W, C)\n","])\n","\n","# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=train_transforms)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=None)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=None)\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n"],"metadata":{"id":"vVlk_Udu3lT4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- see that the image is now randomly shifted cropped and flipped, each time the images enters ther training loop, this operation is happening on the fly while training the model\n"],"metadata":{"id":"4kHtT_s0snjf"}},{"cell_type":"code","source":["images, labels = next(iter(train_dataloader))\n","print(images.min(), images.max())\n","plt.figure(figsize=(2,2))\n","plt.imshow(images[0].numpy().astype(np.uint8))\n","plt.show()"],"metadata":{"id":"d3TQ8yBWsm8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history =model.fit(train_dataloader, epochs=15, validation_data=val_dataloader)\n"],"metadata":{"id":"hOXh8z6nd-VR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19jPWWNeDBgR"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUiqoT9gDafA"},"outputs":[],"source":["acc = np.average(np.argmax(model.predict(test_dataloader),axis=1)==np.argmax(Y_test,axis=1))\n","res6 = pd.DataFrame(\n","          {'Acc' : acc}, index=['cnn from scratch with data augmentation']\n",")\n","pd.concat([res1,res2,res3,res4,res5,res6])"]},{"cell_type":"markdown","metadata":{"id":"fWIwUCktrnE4"},"source":["## ðŸ”§ **YOUR TASK:**\n","- Try to beat the performace of the best network with your own neural network.  \n","- you might want to combine some approaches from above\n","- dont forget to normalize also the testset if you use normalization (which you should use anywayðŸ˜‰)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["<details>\n","  <summary>ðŸ’¡ Click here for a hint:</summary>\n","\n","\n","\n","these transformatins normalize your data to 0- 1 and use augmentation\n","\n","```\n","train_transforms = tr.Compose([\n","    tr.ToPILImage(),               #  Convert numpy array to PIL image\n","    tr.ToTensor(),                 # Convert PIL image to Tensor\n","    tr.RandomHorizontalFlip(),                                                           # Random horizontal flip\n","    tr.RandomRotation(15),                                                              # Random rotation\n","    tr.RandomCrop(32, padding=4),                                                       # Random cropping with padding\n","    tr.Lambda(lambda x: x.permute(1, 2, 0))                     # Scale back to [0, 255] and return to (H, W, C)\n","])\n","val_test_transforms = tr.Compose([\n","    tr.ToPILImage(),               # Converting numpy array to PIL image\n","    tr.ToTensor(),                 # Convert PIL image to Tensor\n","    tr.Lambda(lambda x: x.permute(1, 2, 0))\n","  ])\n","# Convert one-hot labels to class indices\n","Y_train_indices = np.argmax(Y_train, axis=1)\n","Y_val_indices = np.argmax(Y_val, axis=1)\n","Y_test_indices = np.argmax(Y_test, axis=1)\n","# Create datasets\n","train_dataset = NumpyDataset(X_train, Y_train_indices, transform=train_transforms)\n","val_dataset = NumpyDataset(X_val, Y_val_indices, transform=val_test_transforms)\n","test_dataset= NumpyDataset(X_test, Y_test_indices, transform=val_test_transforms)\n","# From the datasets we create DataLoaders\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","```\n","\n","\n","</details>"],"metadata":{"id":"7Xi8zjSNpqhF"}},{"cell_type":"code","source":["### YOUR CODE ###"],"metadata":{"id":"-h461vHFq4wW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"collapsed_sections":["PEIS4WvpsT5t","BSEPI14itYH9","Y7kfDCYsxRVJ","z5rxBKcRdPKR","mi-o6-OagtZf","iDqf88OmQ8mg","wIZH1xpehpOH","qP_oH_fcih0D","XiKVO-uOlMbg"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}