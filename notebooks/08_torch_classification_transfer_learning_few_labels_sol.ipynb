{"cells":[{"cell_type":"markdown","metadata":{"id":"tGBEB7Pvd-0u"},"source":["# Exercise on the value of unsupervised constructed features for training a classifier with few labeled examples\n","\n","To get unsupervised constructed features of an image, we can use a pretrained CNN as feature extractor.\n","\n","We have done this to extract features from 100 Cifar10 images.  As pretrained CNN we use a VGG16 architecture that was trained on ImageNet data and was the second winner of the ImageNet competition in 2014.\n","\n","As a check on the quality of the feature representation of the CIFAR10 data, we will use once the pixel-features and once the VGG-features to train a classifier using this 100 labeled data (on average 10 per class). If the VGG-feature are indeed better than the raw pixel values, we would expect to achieve a better classifier when using the VGG-feature compared to the pixel feature.\n","\n","a) Which accuracy would you expect for a classifier which cannot distinguish between the 10 classes and is only guessing?\n","\n","\n","\n","b) Go through the code in **SECTION 1)** which is used to set-up, train, and evaluate a CNN classifier using the raw pixel features . Discuss your thoughts on the achieved accuracy (e.g. with your neighbor).\n","\n","\n","c) in **SECTION 2)** Now we use the unsupervised constructed VGG features. We want to check, if these VGG features are good enough to train a classifier with only few labeled data and still get a satisfying performance. For this purpose, please complete the code to set up a fully connected NN and run the provided subsequent code to train it and determine its accuracy on the test set. Compare it to the accuracy which we achieve with a RF. Discuss the results (e.g. with your neighbor).\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### ðŸ”‘ **Solution:**"],"metadata":{"id":"KiCfe3maUqOP"}},{"cell_type":"markdown","source":["<details>\n","  <summary>ðŸ”‘ a) </summary>\n","\n","\n","**Solution: 10%**\n","\n","</details>"],"metadata":{"id":"N1XzpR84Uvwi"}},{"cell_type":"markdown","source":["<details>\n","  <summary>ðŸ”‘ b) </summary>\n","\n","\n","**Solution: The accuracy is with around 20% better then guessing but still very bad. However, this is not surprising since the resolution of the images are very low and it is alread by eye quite difficult to distinguish between the classes. Moreover, we have  only very few training examples (only 10 per class), quite bad features (the raw pixel values) and a model with many parameters (around 45k parameter).**\n","\n","</details>"],"metadata":{"id":"aVbNIATBU2q4"}},{"cell_type":"markdown","source":["<details>\n","  <summary>ðŸ”‘ c) </summary>\n","\n","\n","**Solution: For code completion see below. The accuracy of the fcNN on the VGG features is with more than 55% much better than the accuray of the from scratch trained CNN which was 20%. This implies that the VGG-features are quite good and more informative than the raw pixel features. With the RF we achieve a similar performance.**\n","\n","</details>"],"metadata":{"id":"Gip0c306U3jY"}},{"cell_type":"markdown","source":["# Section 1"],"metadata":{"id":"We9h1b63cYNZ"}},{"cell_type":"markdown","metadata":{"id":"PDJ_bt5Rd-02"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85458Tp9d-02"},"outputs":[],"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as imgplot\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from pylab import *\n","\n","import time\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"torch\"\n","import keras\n","import torch # not needed yet\n","\n","print(f'Keras_version: {keras.__version__}')# 3.5.0\n","print(f'torch_version: {torch.__version__}')# 2.5.1+cu121\n","print(f'keras backend: {keras.backend.backend()}')\n","\n","# Keras Building blocks\n","from keras.models import Sequential\n","from keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n","from keras.optimizers import SGD\n","from keras.utils import to_categorical\n","from keras import optimizers\n","\n","import sys"]},{"cell_type":"markdown","metadata":{"id":"MUIuN6nOd-05"},"source":["## CIFAR Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20_9rgUsd-05"},"outputs":[],"source":["#downlad cifar data\n","from keras.datasets import cifar10\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","del [x_test,y_test]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFrSwPeCd-05"},"outputs":[],"source":["#loop over each class label and sample 100 random images over each label and save the idx to subset\n","np.random.seed(seed=222)\n","idx=np.empty(0,dtype=\"int8\")\n","for i in range(0,len(np.unique(y_train))):\n","    idx=np.append(idx,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],100,replace=False))\n","\n","x_train= x_train[idx]\n","y_train= y_train[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0B37tobId-06"},"outputs":[],"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(np.unique(y_train,return_counts=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmF8lJPed-07"},"outputs":[],"source":["#make train valid and test\n","#loop over each class label and sample 100 random images over each label and save the idx to subset\n","np.random.seed(seed=123)\n","idx_train=np.empty(0,dtype=\"int8\")\n","for i in range(0,len(np.unique(y_train))):\n","    idx_train=np.append(idx_train,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],10,replace=False))\n","\n","x_train_new = x_train[idx_train]\n","y_train_new = y_train[idx_train]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqiJHsodd-08"},"outputs":[],"source":["x_test_new=(np.delete(x_train,idx_train,axis=0))\n","y_test_new=(np.delete(y_train,idx_train,axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuXfHWZvd-09"},"outputs":[],"source":["np.random.seed(seed=127)\n","idx_valid=np.empty(0,dtype=\"int8\")\n","for i in range(0,len(np.unique(y_test_new))):\n","    idx_valid=np.append(idx_valid,np.random.choice(np.where((y_test_new[0:len(y_test_new)])==i)[0],10,replace=False))\n","\n","x_valid_new = x_test_new[idx_valid]\n","y_valid_new = y_test_new[idx_valid]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPzf5eiOd-09"},"outputs":[],"source":["x_test_new=(np.delete(x_test_new,idx_valid,axis=0))\n","y_test_new=(np.delete(y_test_new,idx_valid,axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAjRrwBFd-0-"},"outputs":[],"source":["x_train_new = np.reshape(x_train_new, (100,32,32,3))\n","x_valid_new = np.reshape(x_valid_new, (100,32,32,3))\n","x_test_new = np.reshape(x_test_new, (800,32,32,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub4bXw51d-0-"},"outputs":[],"source":["print(np.unique(y_train_new,return_counts=True))\n","print(np.unique(y_valid_new,return_counts=True))\n","print(np.unique(y_test_new,return_counts=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqAPBiZNd-0_"},"outputs":[],"source":["from keras.utils import to_categorical\n","\n","y_train_new=to_categorical(y_train_new,10)\n","y_valid_new=to_categorical(y_valid_new,10)\n","y_test_new=to_categorical(y_test_new,10)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdQXPfHUd-0_"},"outputs":[],"source":["print(x_train_new.shape)\n","print(y_train_new.shape)\n","\n","print(x_valid_new.shape)\n","print(y_valid_new.shape)\n","\n","print(x_test_new.shape)\n","print(y_test_new.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hDsmjn4d-1A"},"outputs":[],"source":["# center and standardize the data\n","X_mean = np.mean( x_train_new, axis = 0)\n","X_std = np.std( x_train_new, axis = 0)\n","\n","x_train_new = (x_train_new - X_mean ) / (X_std + 0.0001)\n","x_valid_new = (x_valid_new - X_mean ) / (X_std + 0.0001)\n","x_test_new = (x_test_new - X_mean ) / (X_std + 0.0001)"]},{"cell_type":"markdown","metadata":{"id":"fia86akMsKz7"},"source":["## Baseline 1: use raw images to train a Random Forest model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjqKjUX0sKz7"},"outputs":[],"source":["# reshape images for rf\n","x_train_rf = x_train_new.reshape(len(x_train_new),32*32*3)\n","x_valid_rf = x_valid_new.reshape(len(x_valid_new),32*32*3)\n","x_test_rf = x_test_new.reshape(len(x_test_new),32*32*3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edd0yO-csKz7"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier()\n","clf.fit(x_train_rf, np.argmax(y_train_new, axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BNvMoNJsKz8"},"outputs":[],"source":["pred=clf.predict(x_test_rf)\n","# get confusion matrix\n","cm = confusion_matrix(np.argmax(y_test_new, axis=1), pred)\n","\n","acc_fc = np.sum(pred == np.argmax(y_test_new, axis=1)) / len(np.argmax(y_test_new, axis=1))\n","print(\"Accuracy = \" , acc_fc)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap='viridis')\n","plt.title('Confusion Matrix, Baseline 1, RF'),plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bmFjrUIxd-1B"},"source":["## Setting up the the CNN classifier based on raw image data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hOO4d4vd-1B"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, BatchNormalization\n","from keras.layers import Convolution2D, MaxPooling2D, Flatten\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6RNdedNd-1B"},"outputs":[],"source":["# here we define hyperparameter of the NN\n","batch_size = 10\n","nb_classes = 10\n","nb_epoch = 30\n","img_rows, img_cols = 32, 32\n","kernel_size = (3, 3)\n","input_shape = (img_rows, img_cols, 3)\n","pool_size = (2, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqjynUYMd-1B"},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Convolution2D(8, kernel_size,padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Convolution2D(16, kernel_size,padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Convolution2D(16,kernel_size,padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=pool_size))\n","\n","model.add(Flatten())\n","model.add(Dense(40))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","model.add(Activation('relu'))\n","model.add(Dense(nb_classes))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4cqyvwZd-1C"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M03zx5y4d-1D"},"outputs":[],"source":["history=model.fit(x_train_new, y_train_new,\n","                  batch_size=10,\n","                  epochs=30,\n","                  verbose=2,\n","                  validation_data=(x_valid_new, y_valid_new),shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"j7kEbIcad-1D"},"source":["### Evaluation of the CNN classifier that was trained on raw image data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYcyO4N4d-1E"},"outputs":[],"source":["pred=model.predict(x_test_new)\n","cm = confusion_matrix(np.argmax(y_test_new,axis=1), np.argmax(pred,axis=1))\n","acc_cnn = np.sum(np.argmax(y_test_new,axis=1)==np.argmax(pred,axis=1))/len(y_test_new)\n","print(\"Accuracy = \" , acc_cnn)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap='viridis')\n","plt.title('Confusion Matrix, Baseline 1, CNN')\n","plt.show();\n"]},{"cell_type":"markdown","source":["# Section 2"],"metadata":{"id":"-H4uzyCTbj5S"}},{"cell_type":"markdown","metadata":{"id":"i5k7FBald-1E"},"source":["## Getting the VGG features for CIFAR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQH8us4Bd-1F"},"outputs":[],"source":["# Downloading embeddings\n","import urllib\n","import os\n","if not os.path.isfile('cifar_EMB_1000.npz'):\n","    urllib.request.urlretrieve(\n","    \"https://www.dropbox.com/s/si287al91c1ls0d/cifar_EMB_1000.npz?dl=1\",\n","    \"cifar_EMB_1000.npz\")\n","%ls -hl cifar_EMB_1000.npz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bux3xwf0d-1F"},"outputs":[],"source":["Data=np.load(\"cifar_EMB_1000.npz\")\n","vgg_features_cifar = Data[\"arr_0\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFUrY5ekd-1F"},"outputs":[],"source":["vgg_features_cifar_train = vgg_features_cifar[idx_train]\n","vgg_features_cifar_test=(np.delete(vgg_features_cifar,idx_train,axis=0))\n","vgg_features_cifar_valid = vgg_features_cifar_test[idx_valid]\n","vgg_features_cifar_test=(np.delete(vgg_features_cifar_test,idx_valid,axis=0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZCEIourd-1G"},"outputs":[],"source":["print(vgg_features_cifar_train.shape)\n","print(vgg_features_cifar_valid.shape)\n","print(vgg_features_cifar_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"D8ym3NgNd-1I"},"source":["## Baseline 2: use VGG feature to train a Random Forest model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04ljtLiwd-1I"},"outputs":[],"source":["clf = RandomForestClassifier()\n","clf.fit(vgg_features_cifar_train,np.argmax(y_train_new, axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMsIVx7zd-1J"},"outputs":[],"source":["pred=clf.predict(vgg_features_cifar_test)\n","# get confusion matrix\n","cm = confusion_matrix(np.argmax(y_test_new, axis=1), pred)\n","\n","acc_vgg_rf = np.sum(pred==np.argmax(y_test_new, axis=1))/len(np.argmax(y_test_new, axis=1))\n","print(\"Accuracy = \" , acc_vgg_rf)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap='viridis')\n","plt.title('Confusion Matrix,VGG  Baseline 2, RF'),plt.show();\n"]},{"cell_type":"markdown","metadata":{"id":"SXkrEaXKd-1G"},"source":["## Setting up the the NN classifier based on VGG feature\n","\n","### ðŸ”§ Your Task:\n","  - fill in the missing Code below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN12Ss4md-1H"},"outputs":[],"source":["model = Sequential()\n","model.add(Dense(200,input_shape=(4096,)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","model.add(Activation('relu'))\n","model.add(Dense(200))\n","\n","# we still need to add the last layers to get the predictions on the 10 classes\n","\n","#######ðŸ”§  your code here  ######\n","\n","\n","\n","####### ðŸ”§ end of your code ######\n"]},{"cell_type":"code","source":["# @title ðŸ”‘ Solution Code { display-mode: \"form\" }\n","\n","### use these layers\n","model.add(Dense(nb_classes))\n","model.add(Activation('softmax'))\n","###"],"metadata":{"id":"TQO7YPOWZu6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziQXpgHtd-1H"},"outputs":[],"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QK45pvVCd-1H"},"outputs":[],"source":["history=model.fit(vgg_features_cifar_train, y_train_new,\n","                  batch_size=10,\n","                  epochs=20,\n","                  verbose=2,\n","                  validation_data=(vgg_features_cifar_valid, y_valid_new),shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"AlHwGiRsd-1I"},"source":["### Evaluation of the NN classifier that was trained on VGG features\n","\n","\n","### ðŸ”§ Your Task:\n","  - fill in the missing Code below, from the predicitons plot the confusion matrix and calculate accuracy\n","\n","  `cm =`\n","  \n"," ` acc_vgg_nn=`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26ZX9Kqdd-1J"},"outputs":[],"source":["pred=model.predict(vgg_features_cifar_test)\n","# get confusion matrix\n","#### we now want to get the confusion matrix for the predictions on the test data\n","\n","#######ðŸ”§  your code here  ######\n","\n","\n","####### ðŸ”§ end of your code ######"]},{"cell_type":"code","source":["# @title ðŸ”‘ Solution Code { display-mode: \"form\" }\n","cm = confusion_matrix(np.argmax(y_test_new,axis=1),np.argmax(pred,axis=1))\n","\n","acc_vgg_nn = np.sum(np.argmax(y_test_new,axis=1)==np.argmax(pred,axis=1))/len(y_test_new)\n","print(\"Accuracy = \" , acc_vgg_nn)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap='viridis')\n","plt.title('Confusion Matrix,VGG  Baseline 2, CNN'),plt.show();"],"metadata":{"id":"Abhwqtp6aUaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Data        |      Model      |  Accuracy\n","------------|-----------------|----------\n","RAW         | Random Forest   | {acc_fc:.4f}\n","RAW         | CNN             | {acc_cnn:.4f}\n","VGG_features| Random Forest   | {acc_vgg_rf:.4f}\n","VGG_features| Neural Network  | {acc_vgg_nn:.4f}\n","\"\"\")\n"],"metadata":{"id":"Awnn-CPidNE_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["tGBEB7Pvd-0u","KiCfe3maUqOP","We9h1b63cYNZ","PDJ_bt5Rd-02","MUIuN6nOd-05","fia86akMsKz7","bmFjrUIxd-1B","-H4uzyCTbj5S","i5k7FBald-1E","D8ym3NgNd-1I"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}