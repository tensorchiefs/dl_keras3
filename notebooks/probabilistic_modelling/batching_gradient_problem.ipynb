{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A subtil error when using y_train.shape (B,) instead of y_train.shape (B, 1) \n",
    "\n",
    "Background: It happened to me that the network did not train beyond predicting the marginal distribution of the target variable. After some investigation, I realized the issue was caused by an incorrect shape of the target variable (y_true). Finally, ChatGPT-o1 suggested the correct way to fix the error. This notebook explains the root cause of the error and how to resolve it.\n",
    "\n",
    "### The Problem: No singleton last dimension\n",
    "The error occurred during a regression task where the network was designed to output parameters of a Gaussian distribution (mean and log standard deviation), with the negative log-likelihood (NLL) used as the loss function. The issue arises when the target variable (y_true) is provided as a 1D array (e.g., shape (B,)) rather than having a singleton last dimension (e.g., shape (B, 1)).\n",
    "\n",
    "This mistake causes broadcasting errors during the computation of the NLL loss, leading to incorrect gradients and poor model training. This problem is not unique to JAX‚Äîit can occur in any deep learning framework, such as PyTorch or TensorFlow, where broadcasting rules apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.__version__ 0.4.26\n",
      "jax.devices() [CpuDevice(id=0)]\n",
      "3.6.0\n",
      "Keras version: 3.6.0\n",
      "Backend: jax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Before importing Keras!\n",
    "import jax\n",
    "from jax import random\n",
    "from numpyro import distributions as dist\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "print(f\"jax.__version__ {jax.__version__}\")\n",
    "cuda_available = any(device.platform == 'gpu' for device in jax.devices())\n",
    "# Attempt to get CUDA version info (platform_version often includes CUDA info)\n",
    "print(f\"jax.devices() {jax.devices()}\")\n",
    "# get GPU Name\n",
    "if (cuda_available):\n",
    "    print(f\"jax.devices()[0].device_kind {jax.devices()[0].device_kind}\")\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Backend: {keras.backend.backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last layer of the network\n",
    "The output of the network has the dimension (B,2). The first dimension is the mean of the Gaussian distribution and the second dimension is the standard deviation. We use JAX to convert it into a distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpyro.distributions.continuous.Normal at 0x17f71f850>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@staticmethod\n",
    "def output_to_gaussian_distribution(out):\n",
    "    mean = out[:, :1]       # first column is mean\n",
    "    log_sd = out[:, 1:]    # last column is log variance\n",
    "    #scale = 1e-3 + stable_softplus(0.05 * out[:, 1:])  # Apply stable softplus to log scale\n",
    "    scale = jnp.exp(log_sd)\n",
    "    return dist.Normal(mean, scale)\n",
    "\n",
    "out = jnp.array([[1.0, 0.1], [2.0, 0.2], [3.0, 0.1]])\n",
    "output_to_gaussian_distribution(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct y_train is (B,1) üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.03002013],\n",
       "       [0.02234398],\n",
       "       [0.06276936]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NLL(y_true, y_pred):\n",
    "    return -output_to_gaussian_distribution(y_pred).log_prob(y_true).mean()\n",
    "\n",
    "y_train = np.array([1.11, 2.1, 3.23]).reshape(-1, 1)\n",
    "jax.grad(NLL)(y_train, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wong y_train is (B,) üëé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.22821394,  0.02564199,  0.3153968 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array([1.11, 2.1, 3.23])\n",
    "jax.grad(NLL)(y_train, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
